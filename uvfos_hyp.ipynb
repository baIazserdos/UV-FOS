{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import models\n",
    "from keras import layers\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = import_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate test set and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 47 # 25% of all samples\n",
    "\n",
    "np.random.shuffle(data)\n",
    "test_data = data[:num_test_samples]\n",
    "data = data[num_test_samples:]\n",
    "\n",
    "mean = data[:,:-6].mean(axis=0) # leave the target variables out of normalization!\n",
    "data[:,:-6] -= mean\n",
    "std = data[:,:-6].std(axis=0)\n",
    "data[:,:-6] /= std\n",
    "\n",
    "test_data[:,:-6] -= mean\n",
    "test_data[:,:-6] /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "### example of hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_sizes = [8, 16, 32, 64] # hidden layer sizes to search through\n",
    "layer2_sizes = [8, 16, 32, 32]\n",
    "\n",
    "def build_model(layer1_size, layer2_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(layer1_size, activation='relu', \n",
    "                           input_shape=(train_X.shape[1],)))\n",
    "    model.add(layers.Dense(layer2_size, activation='relu'))\n",
    "    model.add(layers.Dense(6))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_size= 8, layer2_size= 8\n",
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "layer1_size= 16, layer2_size= 16\n",
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "layer1_size= 32, layer2_size= 32\n",
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n",
      "layer1_size= 64, layer2_size= 32\n",
      "processing fold # 0\n",
      "processing fold # 1\n",
      "processing fold # 2\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "num_val_samples = len(data) // k\n",
    "num_epochs = 500\n",
    "\n",
    "validation_scores_all = []\n",
    "\n",
    "for layer1_size, layer2_size in zip(layer1_sizes, layer2_sizes):\n",
    "    print('layer1_size= %i, layer2_size= %i' % (layer1_size, layer2_size))\n",
    "    validation_scores = []\n",
    "    \n",
    "    for fold in range(k):\n",
    "        validation_data = data[num_val_samples * fold:\n",
    "                               num_val_samples *(fold+1)]\n",
    "        training_data = np.concatenate([data[:num_val_samples * fold],\n",
    "                                        data[num_val_samples * (fold+1):]], axis=0)\n",
    "\n",
    "        train_X = training_data[:,:-6]\n",
    "        train_y = training_data[:,-6:]\n",
    "        val_X = validation_data[:,:-6]\n",
    "        val_y = validation_data[:,-6:]\n",
    "\n",
    "        print('processing fold #', fold)\n",
    "        model = build_model(layer1_size, layer2_size)\n",
    "        train_history = model.fit(train_X, train_y, epochs=num_epochs, \n",
    "                                  validation_data=(val_X, val_y), \n",
    "                                  batch_size=1, verbose=0)\n",
    "\n",
    "        validation_score = model.evaluate(val_X, val_y, verbose=0)\n",
    "        validation_scores.append(validation_score)\n",
    "        \n",
    "    validation_scores_all.append(validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer sizes: (8, 8) \n",
      " average val_mse: 42.738263 average val_mae: 3.966958\n",
      "hidden layer sizes: (16, 16) \n",
      " average val_mse: 39.412308 average val_mae: 3.884672\n",
      "hidden layer sizes: (32, 32) \n",
      " average val_mse: 37.539061 average val_mae: 3.431678\n",
      "hidden layer sizes: (64, 32) \n",
      " average val_mse: 31.151189 average val_mae: 3.339603\n"
     ]
    }
   ],
   "source": [
    "for i, layer_sizes in enumerate(zip(layer1_sizes, layer2_sizes)):\n",
    "    print('hidden layer sizes: %s \\n average val_mse: %2f average val_mae: %2f' % (layer_sizes, np.average(validation_scores_all[i], axis=0)[0], np.average(validation_scores_all[i], axis=0)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
